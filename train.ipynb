{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Administrator/.cache\\torch\\sentence_transformers\\nghuyong_ernie-3.0-nano-zh. Creating a new one with MEAN pooling.\n",
      "Some weights of ErnieModel were not initialized from the model checkpoint at C:\\Users\\Administrator/.cache\\torch\\sentence_transformers\\nghuyong_ernie-3.0-nano-zh and are newly initialized: ['ernie.pooler.dense.weight', 'ernie.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "model = SentenceTransformer('nghuyong/ernie-3.0-nano-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "39\n",
      "35\n",
      "32\n",
      "564\n"
     ]
    }
   ],
   "source": [
    "# 加载数据，构建训练集\n",
    "\n",
    "# 读取文件\n",
    "f = open('./cls.txt', 'r', encoding='utf-8').readlines()\n",
    "f = [i.strip() for i in f]\n",
    "\n",
    "# 将数据转换为字典\n",
    "data = {}\n",
    "for i in f:\n",
    "    t, e = i.split('：')\n",
    "    e = e.split('、')\n",
    "    data[t] = e\n",
    "    print(len(e))\n",
    "\n",
    "# 构建训练集：将每个类别与其中的元素相似度定义为1，与其他类别的元素相似度定义为0\n",
    "train_data = []\n",
    "for k1, v1 in data.items():\n",
    "    for v in v1:\n",
    "        train_data.append(InputExample(texts=[k1, v], label=float(1)))\n",
    "    for k2, v2 in data.items():\n",
    "        if k1 != k2:\n",
    "            for v in v2:\n",
    "                train_data.append(InputExample(texts=[k1, v], label=float(0)))\n",
    "print(len(train_data))\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model)  # 定义损失函数，与查询时使用的方法一致\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=50, show_progress_bar=False)  # 训练模型\n",
    "model.save('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 电池\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "有害垃圾 (Score: 0.7171)\n",
      "干垃圾 (Score: 0.4939)\n",
      "可回收垃圾 (Score: -0.0149)\n",
      "厨余垃圾 (Score: -0.2701)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 外卖盒\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "可回收垃圾 (Score: 0.7201)\n",
      "干垃圾 (Score: 0.1896)\n",
      "有害垃圾 (Score: 0.1494)\n",
      "厨余垃圾 (Score: -0.1899)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 骨头\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "厨余垃圾 (Score: 0.7113)\n",
      "干垃圾 (Score: 0.0334)\n",
      "可回收垃圾 (Score: -0.0730)\n",
      "有害垃圾 (Score: -0.1449)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 化妆品\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "有害垃圾 (Score: 0.6373)\n",
      "干垃圾 (Score: 0.5172)\n",
      "厨余垃圾 (Score: -0.0775)\n",
      "可回收垃圾 (Score: -0.2346)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 纸箱\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "可回收垃圾 (Score: 0.9293)\n",
      "干垃圾 (Score: -0.0017)\n",
      "厨余垃圾 (Score: -0.0717)\n",
      "有害垃圾 (Score: -0.1058)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "import torch\n",
    "\n",
    "\n",
    "corpus = ['干垃圾', '可回收垃圾', '厨余垃圾', '有害垃圾']\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['电池', '外卖盒', '骨头', '化妆品', '纸箱']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    x = [i.cpu().numpy() for i in x]\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "def cls(query):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=4)\n",
    "    result_id, result_score = [], []\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        result_id.append(idx)\n",
    "        result_score.append(score)\n",
    "    result_score = softmax(result_score)\n",
    "    result_label = [corpus[i] for i in result_id]\n",
    "    result = dict(zip(result_label, result_score))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'厨余垃圾': 0.44761857, '干垃圾': 0.1962006, '可回收垃圾': 0.17894478, '有害垃圾': 0.17723598}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls('大树')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
